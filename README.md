# Lightweight LLM fine-tuning for censorship evasion

In recent years, Chinese large language models (LLMs) have made impressive progress, achieving strong performance in various benchmarks. However, a persistent challenge has been the issue of censorship, where responses to certain topics are often overly restricted or biased. In this project, we demonstrate how to fine-tune an open-weight LLM with the goal of reducing excessive censorship and bias while maintaining the modelâ€™s capability as a general-purpose assistant.
