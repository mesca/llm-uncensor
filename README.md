# Lightweight LLM fine-tuning for censorship evasion

In recent years, Chinese large language models (LLMs) have made impressive progress, achieving strong performance in various benchmarks. However, a persistent challenge has been the issue of censorship, where responses to certain topics are often overly restricted or biased. In this project, we demonstrate how to fine-tune an open-weight LLM with the goal of reducing excessive censorship and bias while maintaining the model’s capability as a general-purpose assistant.

To achieve this, we employ a Parameter-Efficient Fine-Tuning (PEFT) technique known as __Low-Rank Adaptation (LoRa)__. This method allows the model to adapt to new training objectives without requiring full fine-tuning of all parameters, making the process significantly more efficient in terms of computation and memory. LoRa has proven especially effective in scenarios where limited resources are available, making it well-suited for experimentation and research.

For practical demonstration, we use the __Qwen2.5-0.5B-Instruct__ model, given its relatively small size and lower computational requirements. This choice allows us to efficiently test and showcase the fine-tuning process. However, it is important to note that the same methodology generalizes to larger models within the Qwen family and beyond, which may yield stronger results at scale.

To evaluate the effects of fine-tuning, we constructed a set of sensitive questions and used a less-biased model (GPT-4.1-mini) as the evaluator to judge the responses. The purpose was to identify whether answers from the fine-tuned model still displayed censorship or bias. While this evaluation methodology is not perfect—since the chosen evaluator itself may carry assumptions, biases, or limitations—it nonetheless provides a practical and scalable way to measure shifts in model behavior. Importantly, this approach offers consistent, automated feedback across many test cases, making it highly valuable for detecting broad patterns of improvement, even if it cannot fully capture all nuances of censorship or bias.

For training, we used the __nbeerbower/GreatFirewall-DPO__ dataset from Hugging Face. This dataset contains 492 queries, each with two responses: a standard model reply, and a preferred response that demonstrates reduced self-censorship. Notably, the dataset includes content in both English and Chinese, enabling fine-tuning across multilingual contexts. By leveraging these preference-based examples, the model learns to align more closely with open, balanced responses.
